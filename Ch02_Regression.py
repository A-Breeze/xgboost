# Extreme Gradient Boosting with XGBoost - DataCamp - January 2020
# Ch2: Regression with XGBoost

#-------------------------------------
# ---- Setup ----
# Import external modules
import xgboost as xgb
import pandas as pd
import numpy as np
from pyprojroot import here
from sklearn import __version__ as skl_version
from sklearn.model_selection import train_test_split  # For sklearn API examples
from sklearn.metrics import mean_squared_error
from sklearn.datasets import load_boston  # For Boston housing data set

# Check they have imported OK
print("xgboost version: " + str(xgb.__version__))
print("numpy version: " + str(np.__version__))
print("pandas version: " + str(pd.__version__))
print("sklearn version: " + str(skl_version))

# Project locations
data_folder_path = here('.') / 'data'

#-------------------------------------
# ---- Notes ----
''' Supervised learning: regression
Common metrics:
    Root mean squared error (RMSE)
    Mean absolute error (MAE) <- not affected as much by very large errors, but lacks nice mathematical properties
Common algorithms:
    Linear regression
    Decision trees
Objective (loss) functions: Quantifies how far off a prediction is from the actual target
    Aim: find the model that yields the minimum value of loss across ALL the (training) data points
    In xgboost:
        reg:squarederror <- for regression problems (previously reg:linear)
        reg:logistic <- for classification when you want the decision, not probability
        binary:logistic <- when you want the probability, rather than the decision
Base learners, examples:
    Trees
    Linear models <- this often isn't helpful, because the linear sum of linear models is another linear model
        And we already know how to solve multivariate linear models (by matrix methods)
        However, if you stop the fitting early, it might provide some sort of regularisation
Regularisation = limit model complexity by adding a term to the objective function. Parameters in XGBoost:
    - gamma = minimum loss reduction allowed for a split to occur
    - alpha = L1 regularisation on LEAF weights (NOT on feature weights), larger alpha => more regularisation
    - lambda = L2 regularisation
'''

#-------------------------------------
# ---- Ex01a: Boston housing using sklearn API ----
# Get data and pre-process
boston_bunch = load_boston()
boston_bunch.keys()  # Available attributes
X_boston = pd.DataFrame(boston_bunch.data, columns=boston_bunch.feature_names)
y_boston = pd.Series(boston_bunch.target, name="MEDV")
print(y_boston[:10]) # Response is continuous
X_train, X_test, y_train, y_test = train_test_split(X_boston, y_boston, test_size=0.2, random_state=123)

# Fit model
xg_reg = xgb.XGBRegressor(  # booster = "gbtree" is used as default
    objective='reg:squarederror',  # For linear regression
    n_estimators=10,  # Number of boosted trees to fit
    random_state=123,  # Can't see that this makes any difference
)  # We see that the metric is still decreasing. This is only an example, not tuned for optimal fitting parameters
_ = xg_reg.fit(  # Suppress returning the object
    X_train, y_train,
    eval_set=[(X_test, y_test)],  # Set this to get evals_result out of the model objects
)

# Check the model metric
preds = xg_reg.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, preds))
print("Manual RMSE: %f" % rmse)
print("RMSE from model object: %f" % xg_reg.evals_result()['validation_0']['rmse'][-1])  # Slightly different

# ---- Ex01b: Same, but using xgboost API ----
# Get data same as above. Then convert:
DM_train = xgb.DMatrix(data=X_train, label=y_train)
DM_test = xgb.DMatrix(data=X_test, label=y_test)

# Fit the model
params = {
    "booster": "gbtree",  # => linear base learners
    "objective": "reg:squarederror",
}
reg2_evals_result = dict()  # To collect the evaluated metric, we need to store it in a variable
xg_reg2 = xgb.train(
    dtrain=DM_train, params=params, num_boost_round=10,
    evals=[(DM_test, "Test_data")], verbose_eval=True,
    evals_result=reg2_evals_result,
)  # We see that the metric is still decreasing. This is only an example, not tuned for optimal fitting parameters
preds2 = xg_reg2.predict(DM_test)
rmse2 = np.sqrt(mean_squared_error(y_test, preds2))
print("Manual RMSE: %f" % rmse2)
print("RMSE from model object: %f" % reg2_evals_result['Test_data']['rmse'][-1])  # The same to this many dps
# This result does *not* match that generated by the sklearn API example.
# Looks like it could be initialising at a different point. Couldn't pin-point the exact difference.

# TODO: Continue course from here
# Another example
df = pd.read_csv('.\\01_Data\\04_ames_housing_trimmed_processed.csv')
df.shape
df.columns # Last column ('SalePrice') must be the response
X_ames, y_ames = df.iloc[:,:-1], df.iloc[:,-1]
housing_dmatrix = xgb.DMatrix(data=X_ames, label=y_ames)
params = {"objective":"reg:linear", "max_depth":4}
cv_results = xgb.cv(dtrain=housing_dmatrix, params=params
                    , nfold=4, num_boost_round=5, metrics="rmse" # Also: "mae"
                    , as_pandas=True, seed=123)
print(cv_results)
(cv_results["test-rmse-mean"]).tail(1) # Final boosting round metric
type(cv_results)

# Example with regularisation
boston_dmatrix = xgb.DMatrix(data=X_boston, label=y_boston) # Data defined above
params = {"objective":"reg:linear", "max_depth":4} # booster = "gbtree" is implied by default
l1_params = [1,10,100] # L1 values to try
rmses_l1 = [] # Initialise list to store results
for reg in l1_params:
    params["alpha"] = reg # L1 values need to go into the 'alpha' parameter
    cv_results = xgb.cv(dtrain=boston_dmatrix, params=params
                    , nfold=4, num_boost_round=5, metrics="rmse"
                    , as_pandas=True, seed=123)
    rmses_l1.append(cv_results["test-rmse-mean"].tail(1).values[0]) # Store final result
print("Best rmse as a function of l1:"); 
print(pd.DataFrame(list(zip(l1_params, rmses_l1)), columns=["l1","rmse"]))
    
# Side note: Common syntax for converting many equal-length lists to a DataFrame:
# pd.DataFrame(list(zip(list_1, list_2)), columns=["list_1","list_2"])
# Uses zip() to get from [1,2,3], [a,b,c] to [1,a],[2,b],[3.c]
# In Python 3, zip() returns a GENERATOR which needs to be cast using list()
    
# Plotting trees
import graphviz as gv
xgb.plot_tree(xg_reg, num_trees=0); plt.show() # First tree
xgb.plot_tree(xg_reg, num_trees=9, rankdir="LR"); plt.show() # Tenth tree, sideways
